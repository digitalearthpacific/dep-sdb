{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac_client import Client\n",
    "from odc.stac import load\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from odc.algo import mask_cleanup\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_points = \"tuvalu_20.gpkg\"\n",
    "\n",
    "gdf = gpd.read_file(in_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_than_50 = gdf[gdf[\"depth\"] > -50]\n",
    "\n",
    "sample = less_than_50.sample(25000)\n",
    "second_sample = less_than_50.sample(25000)\n",
    "sample.explore(column=\"depth\", cmap=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = \"https://earth-search.aws.element84.com/v1\"\n",
    "client = Client.open(catalog)\n",
    "\n",
    "# Get extents of gdf\n",
    "bbox = list(gdf.to_crs(\"epsg:4326\").total_bounds)\n",
    "\n",
    "# Expand the bbox slightly\n",
    "buffer = 0.01\n",
    "bbox[0] = bbox[0] - buffer\n",
    "bbox[1] = bbox[1] - buffer\n",
    "bbox[2] = bbox[2] + buffer\n",
    "bbox[3] = bbox[3] + buffer\n",
    "\n",
    "items = client.search(\n",
    "    collections=[\"sentinel-2-c1-l2a\"],\n",
    "    bbox=bbox,\n",
    "    datetime=\"2024-01/2024-09\",\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 30}},\n",
    ").item_collection()\n",
    "\n",
    "print(f\"Found {len(items)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load(\n",
    "    items,\n",
    "    chunks={},\n",
    "    bbox=bbox,\n",
    "    groupby=\"solar_day\",\n",
    "    measurements=[\n",
    "        \"red\",\n",
    "        \"green\",\n",
    "        \"blue\",\n",
    "        \"nir\",\n",
    "        \"nir09\",\n",
    "        \"swir16\",\n",
    "        \"swir22\",\n",
    "        \"coastal\",\n",
    "        \"rededge1\",\n",
    "        \"rededge2\",\n",
    "        \"rededge3\",\n",
    "        \"scl\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# nodata, cloud shadow, medium cloud, high cloud\n",
    "mask_flags = [1, 3, 8, 9]\n",
    "cloud_mask = ~data.scl.isin(mask_flags)\n",
    "masked = data.where(cloud_mask).drop_vars(\"scl\")\n",
    "\n",
    "scaled = (masked.where(masked != 0) * 0.0001).clip(0, 1)\n",
    "\n",
    "scaled = scaled.compute()\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some indices including NDVI, MNDWI, NDWI\n",
    "scaled[\"ndvi\"] = (scaled.nir - scaled.red) / (scaled.nir + scaled.red)\n",
    "scaled[\"ndwi\"] = (scaled.green - scaled.nir) / (scaled.green + scaled.nir)\n",
    "scaled[\"mndwi\"] = (scaled.green - scaled.swir16) / (scaled.green + scaled.swir16)\n",
    "\n",
    "# Create a single median\n",
    "median = scaled.median(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled[[\"red\", \"green\", \"blue\"]].to_array().plot.imshow(col=\"time\", col_wrap=2, vmin=0, vmax=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median[[\"red\", \"green\", \"blue\"]].to_array().plot.imshow(size=6, vmin=0, vmax=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprojected = sample.to_crs(median.odc.crs)\n",
    "\n",
    "# Convert the geodataframe to an xarray\n",
    "pts_da = sample.assign(x=reprojected.geometry.x, y=reprojected.geometry.y).to_xarray()\n",
    "\n",
    "# Extract values from the EO data onto the points xarray, and convert back to pandas\n",
    "pt_values_i = (\n",
    "    median.sel(pts_da[[\"x\", \"y\"]], method=\"nearest\").squeeze().compute().to_pandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_array = pd.concat([sample, pt_values_i], axis=1)\n",
    "training_array = training_array.drop(\n",
    "    columns=[\n",
    "        \"y\",\n",
    "        \"x\",\n",
    "        \"spatial_ref\",\n",
    "        \"geometry\",\n",
    "        \"index_left\"\n",
    "    ]\n",
    ")\n",
    "# Drop rows where there are any NaNs\n",
    "training_array = training_array.dropna()\n",
    "\n",
    "training_array.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array(training_array)[:, 1:]\n",
    "values = np.array(training_array)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = AdaBoostRegressor()\n",
    "\n",
    "model = regr.fit(training_data, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(len(scaled.time)):\n",
    "    one_time = scaled.isel(time=i)\n",
    "\n",
    "    # Replace nans with -9999\n",
    "    one_time = one_time.fillna(-9999)\n",
    "\n",
    "    stacked_arrays = one_time.to_array().stack(dims=[\"y\", \"x\"]).transpose()\n",
    "\n",
    "    p = model.predict(stacked_arrays)\n",
    "    array = p.reshape(len(masked.y), len(masked.x))\n",
    "    predictions.append(xr.DataArray(\n",
    "        array, coords={\"x\": masked.x, \"y\": masked.y}, dims=[\"y\", \"x\"]\n",
    "    ))\n",
    "\n",
    "print(f\"Completed predicting {len(scaled.time)} time slices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions into an xarray\n",
    "predicted = xr.concat(predictions, dim=scaled.time).to_dataset(name=\"depth\")\n",
    "predicted = predicted.where(cloud_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.depth.plot.imshow(col=\"time\", col_wrap=2, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = predicted.depth.median(\"time\")  # Use mean or median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closer to 1 is better\n",
    "model.score(training_data, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = (bbox[1] + bbox[3])/2, (bbox[0] + bbox[2])/2\n",
    "m = folium.Map(location=coords, zoom_start=12, layer_control=True)\n",
    "\n",
    "visual = median.odc.to_rgba([\"red\", \"green\", \"blue\"], vmin=0, vmax=0.3)\n",
    "visual.odc.add_to(m, name=\"RGB\")\n",
    "\n",
    "predicted.isel(time=0).depth.odc.add_to(m, name=\"Depth\", cmap=\"Blues_r\")\n",
    "average.odc.add_to(m, name=\"Average Depth\", cmap=\"Blues_r\")\n",
    "\n",
    "# Layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_sample_reprojected = second_sample.to_crs(median.odc.crs)\n",
    "pts_da = second_sample.assign(x=second_sample_reprojected.geometry.x, y=second_sample_reprojected.geometry.y).to_xarray()\n",
    "\n",
    "# Extract values from the EO data onto the points xarray, and convert back to pandas\n",
    "compare_depths = (\n",
    "    average.sel(pts_da[[\"x\", \"y\"]], method=\"nearest\").squeeze().compute().to_pandas()\n",
    ").rename(\"depth_computed\")\n",
    "\n",
    "appended = pd.concat([second_sample, compare_depths], axis=1)\n",
    "\n",
    "appended[\"error\"] = appended.depth - appended.depth_computed\n",
    "\n",
    "appended[\"error\"].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water = ((median.mndwi + median.ndwi) > 0)\n",
    "water_filtered = mask_cleanup(water, [(\"opening\", 5)])\n",
    "water_filtered.plot.imshow(size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = average.where(water_filtered)\n",
    "final.plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.odc.write_cog(\"tuvalu_depth_test2.tif\", overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
